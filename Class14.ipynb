{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCI4FT+TvVqRuyrji1Kick",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guilhermelaviola/NeuralNetworksAndDeepLearning/blob/main/Class14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Applications of AutoEncoders**\n",
        "Autoencoder neural networks are powerful unsupervised learning models widely used in big data and IoT environments due to their ability to compress high-dimensional data into compact representations and accurately reconstruct it. This makes them valuable for tasks such as data compression, noise reduction, anomaly detection, image reconstruction, and recommendation systems, especially in bandwidth-limited or unlabeled data scenarios. With support from popular machine learning libraries like TensorFlow, Keras, and PyTorch, autoencoders can be implemented efficiently in languages such as Python, allowing developers with varying levels of mathematical expertise to apply them effectively by carefully tuning model hyperparameters."
      ],
      "metadata": {
        "id": "9Q0oG44zSjxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Example with Python: AutoEncoders with Keras**\n",
        "The following example shows a minimal example of building and training an autoencoder using the MNIST dataset in Python."
      ],
      "metadata": {
        "id": "K8Cfb7ElPi2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing all the necessary libraries and resources:\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist"
      ],
      "metadata": {
        "id": "k_ggNb9q9He4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGGIgU2NShaJ",
        "outputId": "66f786aa-ce25-4ac2-d487-c73996cedbbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/10\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - loss: 0.3478 - val_loss: 0.1615\n",
            "Epoch 2/10\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - loss: 0.1520 - val_loss: 0.1258\n",
            "Epoch 3/10\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.1218 - val_loss: 0.1075\n",
            "Epoch 4/10\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.1057 - val_loss: 0.0967\n",
            "Epoch 5/10\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - loss: 0.0957 - val_loss: 0.0896\n",
            "Epoch 6/10\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.0893 - val_loss: 0.0846\n",
            "Epoch 7/10\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0847 - val_loss: 0.0813\n",
            "Epoch 8/10\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0816 - val_loss: 0.0791\n",
            "Epoch 9/10\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 0.0796 - val_loss: 0.0774\n",
            "Epoch 10/10\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0779 - val_loss: 0.0762\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
          ]
        }
      ],
      "source": [
        "# Loading and preprocessing the MNIST dataset:\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Flattening the images:\n",
        "x_train = x_train.reshape((len(x_train), 28 * 28))\n",
        "x_test = x_test.reshape((len(x_test), 28 * 28))\n",
        "\n",
        "# Defining the autoencoder model:\n",
        "input_dim = 28 * 28\n",
        "encoding_dim = 64   # size of the compressed representation.\n",
        "input_layer = layers.Input(shape=(input_dim,))\n",
        "encoded = layers.Dense(encoding_dim, activation='relu')(input_layer)\n",
        "decoded = layers.Dense(input_dim, activation='sigmoid')(encoded)\n",
        "autoencoder = models.Model(input_layer, decoded)\n",
        "\n",
        "# Compiling the model:\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Training the autoencoder:\n",
        "autoencoder.fit(\n",
        "    x_train,\n",
        "    x_train,\n",
        "    epochs=10,\n",
        "    batch_size=256,\n",
        "    shuffle=True,\n",
        "    validation_data=(x_test, x_test)\n",
        ")\n",
        "\n",
        "# Using the autoencoder to reconstruct test images:\n",
        "reconstructed_images = autoencoder.predict(x_test)"
      ]
    }
  ]
}